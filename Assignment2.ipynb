{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis (2 labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def strip(text):\n",
    "    t = text.lower()\n",
    "    t = re.sub('\\&amp;', ' ', t)\n",
    "    t = re.sub('http\\S+', ' ', t)\n",
    "    t = re.sub('@\\w+', ' ', t)\n",
    "    t = re.sub(\"[^\\w']\", ' ', t)\n",
    "    t = re.sub('\\s+', ' ', t)\n",
    "    return t\n",
    "\n",
    "def explore(df):\n",
    "    print('Observations:', len(df.index))\n",
    "    for label in set(df.label):\n",
    "        print('%.2f'%(len(df[df.label == label])/len(df.index)*100), '%', label)\n",
    "    return\n",
    "\n",
    "def load_dataset(path):\n",
    "    columns = ['id', 'topic', 'label', 'text']\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=columns)\n",
    "    df = df[df.text != 'Not Available']\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.text = df.text.apply(lambda x: strip(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 3422\n",
      "83.02 % positive\n",
      "16.98 % negative\n"
     ]
    }
   ],
   "source": [
    "train_df = load_dataset('/home/kostas/Desktop/Semester 2/text engineering analytics/assignments/assignment2/twitter_datasets/train_2_labels.csv')\n",
    "explore(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 1058\n",
      "73.82 % positive\n",
      "26.18 % negative\n"
     ]
    }
   ],
   "source": [
    "dev_df = load_dataset('/home/kostas/Desktop/Semester 2/text engineering analytics/assignments/assignment2/twitter_datasets/dev_2_labels.csv')\n",
    "explore(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
    "    fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(accuracy)\n",
    "    prec_rec_fscore = precision_recall_fscore_support(y_test, y_pred)\n",
    "    print(prec_rec_fscore)\n",
    "\n",
    "def checker(vectorizers, max_features, stopwords, ngrams):\n",
    "    result = []\n",
    "    for vec in vectorizers:\n",
    "        for n in max_features:\n",
    "            for sw in stopwords:\n",
    "                for ngram in ngrams:\n",
    "                    print('Trying vec=',vec,'max_features=',n,'stopwords=',sw,'ngrams=',ngram)\n",
    "                    vec.set_params(stop_words=sw, max_features=n, ngram_range=ngram)\n",
    "                    pipeline = Pipeline([('vectorizer', vec),('classifier', LogisticRegression())])\n",
    "                    evaluate(pipeline, train_df.text, train_df.label, dev_df.text, dev_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1000 stopwords= None ngrams= (1, 1)\n",
      "0.74763705104\n",
      "(array([ 0.54716981,  0.76995798]), array([ 0.20938628,  0.93854033]), array([ 0.30287206,  0.84593191]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1000 stopwords= None ngrams= (1, 2)\n",
      "0.750472589792\n",
      "(array([ 0.55371901,  0.77588047]), array([ 0.24187726,  0.93085787]), array([ 0.33668342,  0.84633295]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1000 stopwords= None ngrams= (1, 3)\n",
      "0.740075614367\n",
      "(array([ 0.50793651,  0.77145923]), array([ 0.23104693,  0.9206146 ]), array([ 0.31761787,  0.83946293]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1000 stopwords= english ngrams= (1, 1)\n",
      "0.75236294896\n",
      "(array([ 0.6056338 ,  0.76291793]), array([ 0.15523466,  0.96414853]), array([ 0.24712644,  0.85180995]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1000 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 0.54411765,  0.75757576]), array([ 0.13357401,  0.9603073 ]), array([ 0.21449275,  0.84697911]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1000 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.53846154,  0.75629406]), array([ 0.12635379,  0.96158771]), array([ 0.20467836,  0.84667418]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1500 stopwords= None ngrams= (1, 1)\n",
      "0.757088846881\n",
      "(array([ 0.59090909,  0.77637131]), array([ 0.23465704,  0.94238156]), array([ 0.33591731,  0.85135917]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1500 stopwords= None ngrams= (1, 2)\n",
      "0.74763705104\n",
      "(array([ 0.54032258,  0.7751606 ]), array([ 0.24187726,  0.92701665]), array([ 0.33416459,  0.84431487]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1500 stopwords= None ngrams= (1, 3)\n",
      "0.745746691871\n",
      "(array([ 0.53333333,  0.77292111]), array([ 0.23104693,  0.92829706]), array([ 0.32241814,  0.84351367]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1500 stopwords= english ngrams= (1, 1)\n",
      "0.757088846881\n",
      "(array([ 0.625     ,  0.76789366]), array([ 0.18050542,  0.96158771]), array([ 0.28011204,  0.85389426]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1500 stopwords= english ngrams= (1, 2)\n",
      "0.758034026465\n",
      "(array([ 0.63291139,  0.76813075]), array([ 0.18050542,  0.96286812]), array([ 0.28089888,  0.85454545]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 1500 stopwords= english ngrams= (1, 3)\n",
      "0.756143667297\n",
      "(array([ 0.62025316,  0.7671093 ]), array([ 0.17689531,  0.96158771]), array([ 0.2752809 ,  0.85340909]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2000 stopwords= None ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.58878505,  0.77497371]), array([ 0.22743682,  0.94366197]), array([ 0.328125  ,  0.85103926]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2000 stopwords= None ngrams= (1, 2)\n",
      "0.749527410208\n",
      "(array([ 0.55263158,  0.77330508]), array([ 0.22743682,  0.9346991 ]), array([ 0.32225064,  0.84637681]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2000 stopwords= None ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74763705104\n",
      "(array([ 0.54464286,  0.77167019]), array([ 0.22021661,  0.9346991 ]), array([ 0.31362468,  0.84539664]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2000 stopwords= english ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.62666667,  0.76602238]), array([ 0.16967509,  0.96414853]), array([ 0.26704545,  0.8537415 ]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2000 stopwords= english ngrams= (1, 2)\n",
      "0.758979206049\n",
      "(array([ 0.64102564,  0.76836735]), array([ 0.18050542,  0.96414853]), array([ 0.28169014,  0.85519591]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2000 stopwords= english ngrams= (1, 3)\n",
      "0.754253308129\n",
      "(array([ 0.60759494,  0.76608784]), array([ 0.1732852,  0.9603073]), array([ 0.26966292,  0.85227273]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2500 stopwords= None ngrams= (1, 1)\n",
      "0.758034026465\n",
      "(array([ 0.60194175,  0.77486911]), array([ 0.22382671,  0.9475032 ]), array([ 0.32631579,  0.85253456]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2500 stopwords= None ngrams= (1, 2)\n",
      "0.753308128544\n",
      "(array([ 0.56896552,  0.77600849]), array([ 0.23826715,  0.93597951]), array([ 0.33587786,  0.84852002]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2500 stopwords= None ngrams= (1, 3)\n",
      "0.758034026465\n",
      "(array([ 0.59130435,  0.77836691]), array([ 0.24548736,  0.93982074]), array([ 0.34693878,  0.85150812]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2500 stopwords= english ngrams= (1, 1)\n",
      "0.757088846881\n",
      "(array([ 0.63513514,  0.76626016]), array([ 0.16967509,  0.96542894]), array([ 0.26780627,  0.85439093]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2500 stopwords= english ngrams= (1, 2)\n",
      "0.757088846881\n",
      "(array([ 0.63157895,  0.76680244]), array([ 0.1732852 ,  0.96414853]), array([ 0.27195467,  0.85422575]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 2500 stopwords= english ngrams= (1, 3)\n",
      "0.755198487713\n",
      "(array([ 0.61538462,  0.76632653]), array([ 0.1732852 ,  0.96158771]), array([ 0.27042254,  0.85292447]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3000 stopwords= None ngrams= (1, 1)\n",
      "0.75236294896\n",
      "(array([ 0.57731959,  0.77003122]), array([ 0.20216606,  0.9475032 ]), array([ 0.29946524,  0.84959816]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3000 stopwords= None ngrams= (1, 2)\n",
      "0.757088846881\n",
      "(array([ 0.59433962,  0.77521008]), array([ 0.22743682,  0.94494238]), array([ 0.32898172,  0.85170225]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3000 stopwords= None ngrams= (1, 3)\n",
      "0.754253308129\n",
      "(array([ 0.57391304,  0.77624602]), array([ 0.23826715,  0.93725992]), array([ 0.33673469,  0.84918794]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3000 stopwords= english ngrams= (1, 1)\n",
      "0.758034026465\n",
      "(array([ 0.65217391,  0.76541962]), array([ 0.16245487,  0.96927017]), array([ 0.26011561,  0.85536723]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3000 stopwords= english ngrams= (1, 2)\n",
      "0.756143667297\n",
      "(array([ 0.62337662,  0.76656473]), array([ 0.1732852 ,  0.96286812]), array([ 0.27118644,  0.85357548]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3000 stopwords= english ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755198487713\n",
      "(array([ 0.61538462,  0.76632653]), array([ 0.1732852 ,  0.96158771]), array([ 0.27042254,  0.85292447]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3500 stopwords= None ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.59793814,  0.77211238]), array([ 0.20938628,  0.95006402]), array([ 0.31016043,  0.85189437]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3500 stopwords= None ngrams= (1, 2)\n",
      "0.756143667297\n",
      "(array([ 0.59047619,  0.77439664]), array([ 0.22382671,  0.94494238]), array([ 0.32460733,  0.85121107]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3500 stopwords= None ngrams= (1, 3)\n",
      "0.756143667297\n",
      "(array([ 0.58715596,  0.77555321]), array([ 0.23104693,  0.94238156]), array([ 0.33160622,  0.85086705]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3500 stopwords= english ngrams= (1, 1)\n",
      "0.757088846881\n",
      "(array([ 0.65151515,  0.7641129 ]), array([ 0.15523466,  0.97055058]), array([ 0.25072886,  0.85504794]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3500 stopwords= english ngrams= (1, 2)\n",
      "0.758034026465\n",
      "(array([ 0.63636364,  0.7675841 ]), array([ 0.17689531,  0.96414853]), array([ 0.27683616,  0.85471056]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 3500 stopwords= english ngrams= (1, 3)\n",
      "0.758034026465\n",
      "(array([ 0.63636364,  0.7675841 ]), array([ 0.17689531,  0.96414853]), array([ 0.27683616,  0.85471056]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4000 stopwords= None ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.6043956 ,  0.77042399]), array([ 0.19855596,  0.95390525]), array([ 0.29891304,  0.85240275]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4000 stopwords= None ngrams= (1, 2)\n",
      "0.756143667297\n",
      "(array([ 0.59047619,  0.77439664]), array([ 0.22382671,  0.94494238]), array([ 0.32460733,  0.85121107]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4000 stopwords= None ngrams= (1, 3)\n",
      "0.755198487713\n",
      "(array([ 0.58181818,  0.77531646]), array([ 0.23104693,  0.94110115]), array([ 0.33074935,  0.85020243]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4000 stopwords= english ngrams= (1, 1)\n",
      "0.755198487713\n",
      "(array([ 0.63636364,  0.76310484]), array([ 0.15162455,  0.96927017]), array([ 0.24489796,  0.85391991]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4000 stopwords= english ngrams= (1, 2)\n",
      "0.758034026465\n",
      "(array([ 0.64788732,  0.76595745]), array([ 0.16606498,  0.96798976]), array([ 0.26436782,  0.85520362]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4000 stopwords= english ngrams= (1, 3)\n",
      "0.756143667297\n",
      "(array([ 0.62025316,  0.7671093 ]), array([ 0.17689531,  0.96158771]), array([ 0.2752809 ,  0.85340909]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4500 stopwords= None ngrams= (1, 1)\n",
      "0.755198487713\n",
      "(array([ 0.59782609,  0.77018634]), array([ 0.19855596,  0.95262484]), array([ 0.29810298,  0.85174585]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4500 stopwords= None ngrams= (1, 2)\n",
      "0.751417769376\n",
      "(array([ 0.56730769,  0.77148847]), array([ 0.21299639,  0.94238156]), array([ 0.30971129,  0.84841499]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4500 stopwords= None ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753308128544\n",
      "(array([ 0.5754717 ,  0.77310924]), array([ 0.22021661,  0.94238156]), array([ 0.31853786,  0.84939411]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4500 stopwords= english ngrams= (1, 1)\n",
      "0.754253308129\n",
      "(array([ 0.63076923,  0.76233635]), array([ 0.14801444,  0.96927017]), array([ 0.23976608,  0.85343856]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4500 stopwords= english ngrams= (1, 2)\n",
      "0.757088846881\n",
      "(array([ 0.64285714,  0.76518219]), array([ 0.16245487,  0.96798976]), array([ 0.25936599,  0.85472018]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 4500 stopwords= english ngrams= (1, 3)\n",
      "0.758034026465\n",
      "(array([ 0.64      ,  0.76703967]), array([ 0.1732852 ,  0.96542894]), array([ 0.27272727,  0.85487528]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5000 stopwords= None ngrams= (1, 1)\n",
      "0.754253308129\n",
      "(array([ 0.59550562,  0.76883385]), array([ 0.19133574,  0.95390525]), array([ 0.28961749,  0.85142857]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5000 stopwords= None ngrams= (1, 2)\n",
      "0.750472589792\n",
      "(array([ 0.56565657,  0.76955162]), array([ 0.20216606,  0.94494238]), array([ 0.29787234,  0.84827586]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5000 stopwords= None ngrams= (1, 3)\n",
      "0.749527410208\n",
      "(array([ 0.55769231,  0.77044025]), array([ 0.20938628,  0.94110115]), array([ 0.30446194,  0.84726225]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5000 stopwords= english ngrams= (1, 1)\n",
      "0.755198487713\n",
      "(array([ 0.640625  ,  0.76257545]), array([ 0.14801444,  0.97055058]), array([ 0.24046921,  0.85408451]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5000 stopwords= english ngrams= (1, 2)\n",
      "0.759924385633\n",
      "(array([ 0.66197183,  0.76697062]), array([ 0.16967509,  0.96927017]), array([ 0.27011494,  0.85633484]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5000 stopwords= english ngrams= (1, 3)\n",
      "0.755198487713\n",
      "(array([ 0.625     ,  0.76470588]), array([ 0.16245487,  0.96542894]), array([ 0.25787966,  0.85342388]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5500 stopwords= None ngrams= (1, 1)\n",
      "0.753308128544\n",
      "(array([ 0.58888889,  0.76859504]), array([ 0.19133574,  0.95262484]), array([ 0.28882834,  0.85077187]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5500 stopwords= None ngrams= (1, 2)\n",
      "0.750472589792\n",
      "(array([ 0.56565657,  0.76955162]), array([ 0.20216606,  0.94494238]), array([ 0.29787234,  0.84827586]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5500 stopwords= None ngrams= (1, 3)\n",
      "0.75236294896\n",
      "(array([ 0.57281553,  0.77172775]), array([ 0.21299639,  0.94366197]), array([ 0.31052632,  0.84907834]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5500 stopwords= english ngrams= (1, 1)\n",
      "0.757088846881\n",
      "(array([ 0.65625   ,  0.76358149]), array([ 0.15162455,  0.97183099]), array([ 0.24633431,  0.85521127]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5500 stopwords= english ngrams= (1, 2)\n",
      "0.759924385633\n",
      "(array([ 0.66666667,  0.76643074]), array([ 0.16606498,  0.97055058]), array([ 0.26589595,  0.85649718]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 5500 stopwords= english ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758979206049\n",
      "(array([ 0.65714286,  0.76619433]), array([ 0.16606498,  0.96927017]), array([ 0.26512968,  0.85585076]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6000 stopwords= None ngrams= (1, 1)\n",
      "0.751417769376\n",
      "(array([ 0.57954545,  0.76701031]), array([ 0.18411552,  0.95262484]), array([ 0.27945205,  0.84980011]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6000 stopwords= None ngrams= (1, 2)\n",
      "0.749527410208\n",
      "(array([ 0.56122449,  0.76875   ]), array([ 0.19855596,  0.94494238]), array([ 0.29333333,  0.84778863]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6000 stopwords= None ngrams= (1, 3)\n",
      "0.750472589792\n",
      "(array([ 0.5631068 ,  0.77068063]), array([ 0.20938628,  0.94238156]), array([ 0.30526316,  0.84792627]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6000 stopwords= english ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.65079365,  0.76281407]), array([ 0.14801444,  0.97183099]), array([ 0.24117647,  0.85472973]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6000 stopwords= english ngrams= (1, 2)\n",
      "0.757088846881\n",
      "(array([ 0.64705882,  0.76464646]), array([ 0.15884477,  0.96927017]), array([ 0.25507246,  0.85488425]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6000 stopwords= english ngrams= (1, 3)\n",
      "0.758979206049\n",
      "(array([ 0.65277778,  0.76673428]), array([ 0.16967509,  0.96798976]), array([ 0.26934097,  0.85568761]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6500 stopwords= None ngrams= (1, 1)\n",
      "0.751417769376\n",
      "(array([ 0.58139535,  0.76646091]), array([ 0.18050542,  0.95390525]), array([ 0.27548209,  0.84997148]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6500 stopwords= None ngrams= (1, 2)\n",
      "0.75236294896\n",
      "(array([ 0.57575758,  0.77059437]), array([ 0.20577617,  0.94622279]), array([ 0.30319149,  0.84942529]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6500 stopwords= None ngrams= (1, 3)\n",
      "0.751417769376\n",
      "(array([ 0.57      ,  0.77035491]), array([ 0.20577617,  0.94494238]), array([ 0.30238727,  0.84876366]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6500 stopwords= english ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.66101695,  0.76176176]), array([ 0.14079422,  0.97439181]), array([ 0.23214286,  0.85505618]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6500 stopwords= english ngrams= (1, 2)\n",
      "0.759924385633\n",
      "(array([ 0.68253968,  0.76482412]), array([ 0.15523466,  0.97439181]), array([ 0.25294118,  0.85698198]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 6500 stopwords= english ngrams= (1, 3)\n",
      "0.760869565217\n",
      "(array([ 0.66666667,  0.76774848]), array([ 0.1732852 ,  0.96927017]), array([ 0.27507163,  0.85681947]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7000 stopwords= None ngrams= (1, 1)\n",
      "0.75236294896\n",
      "(array([ 0.5862069 ,  0.76725026]), array([ 0.18411552,  0.95390525]), array([ 0.28021978,  0.85045662]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7000 stopwords= None ngrams= (1, 2)\n",
      "0.751417769376\n",
      "(array([ 0.57291667,  0.76923077]), array([ 0.19855596,  0.9475032 ]), array([ 0.29490617,  0.84911073]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7000 stopwords= None ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75236294896\n",
      "(array([ 0.57894737,  0.7694704 ]), array([ 0.19855596,  0.94878361]), array([ 0.29569892,  0.84977064]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7000 stopwords= english ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.66101695,  0.76176176]), array([ 0.14079422,  0.97439181]), array([ 0.23214286,  0.85505618]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7000 stopwords= english ngrams= (1, 2)\n",
      "0.758034026465\n",
      "(array([ 0.67213115,  0.76328987]), array([ 0.14801444,  0.97439181]), array([ 0.24260355,  0.856018  ]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7000 stopwords= english ngrams= (1, 3)\n",
      "0.760869565217\n",
      "(array([ 0.67142857,  0.76720648]), array([ 0.16967509,  0.97055058]), array([ 0.27089337,  0.85698135]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7500 stopwords= None ngrams= (1, 1)\n",
      "0.750472589792\n",
      "(array([ 0.57647059,  0.76567318]), array([ 0.17689531,  0.95390525]), array([ 0.27071823,  0.84948689]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7500 stopwords= None ngrams= (1, 2)\n",
      "0.75236294896\n",
      "(array([ 0.57894737,  0.7694704 ]), array([ 0.19855596,  0.94878361]), array([ 0.29569892,  0.84977064]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7500 stopwords= None ngrams= (1, 3)\n",
      "0.753308128544\n",
      "(array([ 0.58333333,  0.77027027]), array([ 0.20216606,  0.94878361]), array([ 0.3002681 ,  0.85025818]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7500 stopwords= english ngrams= (1, 1)\n",
      "0.757088846881\n",
      "(array([ 0.67241379,  0.762     ]), array([ 0.14079422,  0.97567222]), array([ 0.23283582,  0.85569905]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7500 stopwords= english ngrams= (1, 2)\n",
      "0.757088846881\n",
      "(array([ 0.66666667,  0.76252505]), array([ 0.14440433,  0.97439181]), array([ 0.23738872,  0.85553682]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 7500 stopwords= english ngrams= (1, 3)\n",
      "0.758034026465\n",
      "(array([ 0.65671642,  0.76488396]), array([ 0.15884477,  0.97055058]), array([ 0.25581395,  0.85553047]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8000 stopwords= None ngrams= (1, 1)\n",
      "0.749527410208\n",
      "(array([ 0.57142857,  0.76488706]), array([ 0.1732852 ,  0.95390525]), array([ 0.26592798,  0.84900285]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8000 stopwords= None ngrams= (1, 2)\n",
      "0.750472589792\n",
      "(array([ 0.56842105,  0.76843198]), array([ 0.19494585,  0.9475032 ]), array([ 0.29032258,  0.84862385]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8000 stopwords= None ngrams= (1, 3)\n",
      "0.75236294896\n",
      "(array([ 0.57894737,  0.7694704 ]), array([ 0.19855596,  0.94878361]), array([ 0.29569892,  0.84977064]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8000 stopwords= english ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.66666667,  0.76123876]), array([ 0.13718412,  0.97567222]), array([ 0.22754491,  0.85521886]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8000 stopwords= english ngrams= (1, 2)\n",
      "0.755198487713\n",
      "(array([ 0.65517241,  0.761     ]), array([ 0.13718412,  0.97439181]), array([ 0.22686567,  0.85457608]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8000 stopwords= english ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756143667297\n",
      "(array([ 0.64615385,  0.7633434 ]), array([ 0.15162455,  0.97055058]), array([ 0.24561404,  0.85456595]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8500 stopwords= None ngrams= (1, 1)\n",
      "0.74763705104\n",
      "(array([ 0.55952381,  0.76386037]), array([ 0.16967509,  0.95262484]), array([ 0.26038781,  0.84786325]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8500 stopwords= None ngrams= (1, 2)\n",
      "0.75236294896\n",
      "(array([ 0.57894737,  0.7694704 ]), array([ 0.19855596,  0.94878361]), array([ 0.29569892,  0.84977064]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8500 stopwords= None ngrams= (1, 3)\n",
      "0.753308128544\n",
      "(array([ 0.58695652,  0.76915114]), array([ 0.19494585,  0.95134443]), array([ 0.29268293,  0.85060103]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8500 stopwords= english ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.66666667,  0.76123876]), array([ 0.13718412,  0.97567222]), array([ 0.22754491,  0.85521886]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8500 stopwords= english ngrams= (1, 2)\n",
      "0.755198487713\n",
      "(array([ 0.65517241,  0.761     ]), array([ 0.13718412,  0.97439181]), array([ 0.22686567,  0.85457608]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 8500 stopwords= english ngrams= (1, 3)\n",
      "0.755198487713\n",
      "(array([ 0.64516129,  0.76204819]), array([ 0.14440433,  0.97183099]), array([ 0.2359882 ,  0.85424873]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 9000 stopwords= None ngrams= (1, 1)\n",
      "0.74763705104\n",
      "(array([ 0.55952381,  0.76386037]), array([ 0.16967509,  0.95262484]), array([ 0.26038781,  0.84786325]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 9000 stopwords= None ngrams= (1, 2)\n",
      "0.75236294896\n",
      "(array([ 0.58064516,  0.76891192]), array([ 0.19494585,  0.95006402]), array([ 0.29189189,  0.84994273]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 9000 stopwords= None ngrams= (1, 3)\n",
      "0.749527410208\n",
      "(array([ 0.56382979,  0.76763485]), array([ 0.19133574,  0.9475032 ]), array([ 0.28571429,  0.84813754]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 9000 stopwords= english ngrams= (1, 1)\n",
      "0.756143667297\n",
      "(array([ 0.66666667,  0.76123876]), array([ 0.13718412,  0.97567222]), array([ 0.22754491,  0.85521886]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 9000 stopwords= english ngrams= (1, 2)\n",
      "0.756143667297\n",
      "(array([ 0.66666667,  0.76123876]), array([ 0.13718412,  0.97567222]), array([ 0.22754491,  0.85521886]), array([277, 781]))\n",
      "Trying vec= CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) max_features= 9000 stopwords= english ngrams= (1, 3)\n",
      "0.757088846881\n",
      "(array([ 0.67241379,  0.762     ]), array([ 0.14079422,  0.97567222]), array([ 0.23283582,  0.85569905]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1000 stopwords= None ngrams= (1, 1)\n",
      "0.745746691871\n",
      "(array([ 0.65384615,  0.74806202]), array([ 0.06137184,  0.98847631]), array([ 0.11221122,  0.85162714]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1000 stopwords= None ngrams= (1, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746691871456\n",
      "(array([ 0.69565217,  0.74782609]), array([ 0.05776173,  0.99103713]), array([ 0.10666667,  0.85242291]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1000 stopwords= None ngrams= (1, 3)\n",
      "0.74763705104\n",
      "(array([ 0.75      ,  0.74759152]), array([ 0.05415162,  0.99359795]), array([ 0.1010101 ,  0.85321605]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1000 stopwords= english ngrams= (1, 1)\n",
      "0.745746691871\n",
      "(array([ 0.9       ,  0.74427481]), array([ 0.03249097,  0.99871959]), array([ 0.06271777,  0.8529251 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1000 stopwords= english ngrams= (1, 2)\n",
      "0.746691871456\n",
      "(array([ 0.90909091,  0.74498567]), array([ 0.03610108,  0.99871959]), array([ 0.06944444,  0.85339168]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1000 stopwords= english ngrams= (1, 3)\n",
      "0.74763705104\n",
      "(array([ 0.91666667,  0.7456979 ]), array([ 0.03971119,  0.99871959]), array([ 0.07612457,  0.85385878]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1500 stopwords= None ngrams= (1, 1)\n",
      "0.745746691871\n",
      "(array([ 0.66666667,  0.74758221]), array([ 0.05776173,  0.98975672]), array([ 0.10631229,  0.85179063]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1500 stopwords= None ngrams= (1, 2)\n",
      "0.744801512287\n",
      "(array([ 0.70588235,  0.74543708]), array([ 0.0433213 ,  0.99359795]), array([ 0.08163265,  0.8518112 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1500 stopwords= None ngrams= (1, 3)\n",
      "0.745746691871\n",
      "(array([ 0.7       ,  0.74662813]), array([ 0.05054152,  0.99231754]), array([ 0.09427609,  0.85211655]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1500 stopwords= english ngrams= (1, 1)\n",
      "0.743856332703\n",
      "(array([ 0.875     ,  0.74285714]), array([ 0.02527076,  0.99871959]), array([ 0.04912281,  0.85199345]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1500 stopwords= english ngrams= (1, 2)\n",
      "0.745746691871\n",
      "(array([ 0.9       ,  0.74427481]), array([ 0.03249097,  0.99871959]), array([ 0.06271777,  0.8529251 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 1500 stopwords= english ngrams= (1, 3)\n",
      "0.744801512287\n",
      "(array([ 0.88888889,  0.7435653 ]), array([ 0.02888087,  0.99871959]), array([ 0.05594406,  0.85245902]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2000 stopwords= None ngrams= (1, 1)\n",
      "0.745746691871\n",
      "(array([ 0.68181818,  0.74710425]), array([ 0.05415162,  0.99103713]), array([ 0.10033445,  0.85195377]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2000 stopwords= None ngrams= (1, 2)\n",
      "0.745746691871\n",
      "(array([ 0.75      ,  0.74568138]), array([ 0.0433213 ,  0.99487836]), array([ 0.08191126,  0.85244103]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2000 stopwords= None ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746691871456\n",
      "(array([ 0.8       ,  0.74592522]), array([ 0.0433213 ,  0.99615877]), array([ 0.08219178,  0.85307018]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2000 stopwords= english ngrams= (1, 1)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2000 stopwords= english ngrams= (1, 2)\n",
      "0.745746691871\n",
      "(array([ 0.9       ,  0.74427481]), array([ 0.03249097,  0.99871959]), array([ 0.06271777,  0.8529251 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2000 stopwords= english ngrams= (1, 3)\n",
      "0.744801512287\n",
      "(array([ 0.88888889,  0.7435653 ]), array([ 0.02888087,  0.99871959]), array([ 0.05594406,  0.85245902]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2500 stopwords= None ngrams= (1, 1)\n",
      "0.745746691871\n",
      "(array([ 0.68181818,  0.74710425]), array([ 0.05415162,  0.99103713]), array([ 0.10033445,  0.85195377]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2500 stopwords= None ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.66666667,  0.74400767]), array([ 0.03610108,  0.99359795]), array([ 0.06849315,  0.85087719]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2500 stopwords= None ngrams= (1, 3)\n",
      "0.744801512287\n",
      "(array([ 0.76923077,  0.74449761]), array([ 0.03610108,  0.99615877]), array([ 0.06896552,  0.85213582]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2500 stopwords= english ngrams= (1, 1)\n",
      "0.743856332703\n",
      "(array([ 0.875     ,  0.74285714]), array([ 0.02527076,  0.99871959]), array([ 0.04912281,  0.85199345]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2500 stopwords= english ngrams= (1, 2)\n",
      "0.744801512287\n",
      "(array([ 1.        ,  0.74310181]), array([ 0.02527076,  1.        ]), array([ 0.04929577,  0.85262009]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 2500 stopwords= english ngrams= (1, 3)\n",
      "0.744801512287\n",
      "(array([ 1.        ,  0.74310181]), array([ 0.02527076,  1.        ]), array([ 0.04929577,  0.85262009]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3000 stopwords= None ngrams= (1, 1)\n",
      "0.745746691871\n",
      "(array([ 0.75      ,  0.74568138]), array([ 0.0433213 ,  0.99487836]), array([ 0.08191126,  0.85244103]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3000 stopwords= None ngrams= (1, 2)\n",
      "0.741965973535\n",
      "(array([ 0.66666667,  0.74282983]), array([ 0.02888087,  0.99487836]), array([ 0.05536332,  0.85057471]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3000 stopwords= None ngrams= (1, 3)\n",
      "0.743856332703\n",
      "(array([ 0.71428571,  0.74425287]), array([ 0.03610108,  0.99487836]), array([ 0.06872852,  0.85150685]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3000 stopwords= english ngrams= (1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743856332703\n",
      "(array([ 0.875     ,  0.74285714]), array([ 0.02527076,  0.99871959]), array([ 0.04912281,  0.85199345]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3000 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 1.        ,  0.74239544]), array([ 0.02166065,  1.        ]), array([ 0.04240283,  0.85215494]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3000 stopwords= english ngrams= (1, 3)\n",
      "0.744801512287\n",
      "(array([ 1.        ,  0.74310181]), array([ 0.02527076,  1.        ]), array([ 0.04929577,  0.85262009]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3500 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.70588235,  0.74543708]), array([ 0.0433213 ,  0.99359795]), array([ 0.08163265,  0.8518112 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3500 stopwords= None ngrams= (1, 2)\n",
      "0.741965973535\n",
      "(array([ 0.66666667,  0.74282983]), array([ 0.02888087,  0.99487836]), array([ 0.05536332,  0.85057471]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3500 stopwords= None ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.69230769,  0.74354067]), array([ 0.03249097,  0.99487836]), array([ 0.06206897,  0.85104053]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3500 stopwords= english ngrams= (1, 1)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3500 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 1.        ,  0.74239544]), array([ 0.02166065,  1.        ]), array([ 0.04240283,  0.85215494]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 3500 stopwords= english ngrams= (1, 3)\n",
      "0.744801512287\n",
      "(array([ 1.        ,  0.74310181]), array([ 0.02527076,  1.        ]), array([ 0.04929577,  0.85262009]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=3500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4000 stopwords= None ngrams= (1, 1)\n",
      "0.743856332703\n",
      "(array([ 0.6875    ,  0.74472169]), array([ 0.03971119,  0.99359795]), array([ 0.07508532,  0.85134394]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4000 stopwords= None ngrams= (1, 2)\n",
      "0.740075614367\n",
      "(array([ 0.6       ,  0.74141221]), array([ 0.02166065,  0.99487836]), array([ 0.04181185,  0.84964461]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4000 stopwords= None ngrams= (1, 3)\n",
      "0.741020793951\n",
      "(array([ 0.63636364,  0.74212034]), array([ 0.02527076,  0.99487836]), array([ 0.04861111,  0.85010941]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4000 stopwords= english ngrams= (1, 1)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4000 stopwords= english ngrams= (1, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4000 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4500 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.70588235,  0.74543708]), array([ 0.0433213 ,  0.99359795]), array([ 0.08163265,  0.8518112 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4500 stopwords= None ngrams= (1, 2)\n",
      "0.741020793951\n",
      "(array([ 0.66666667,  0.74165872]), array([ 0.02166065,  0.99615877]), array([ 0.04195804,  0.85027322]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4500 stopwords= None ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.72727273,  0.74307545]), array([ 0.02888087,  0.99615877]), array([ 0.05555556,  0.8512035 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4500 stopwords= english ngrams= (1, 1)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4500 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 1.        ,  0.74239544]), array([ 0.02166065,  1.        ]), array([ 0.04240283,  0.85215494]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 4500 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5000 stopwords= None ngrams= (1, 1)\n",
      "0.743856332703\n",
      "(array([ 0.6875    ,  0.74472169]), array([ 0.03971119,  0.99359795]), array([ 0.07508532,  0.85134394]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5000 stopwords= None ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.77777778,  0.74261201]), array([ 0.02527076,  0.99743918]), array([ 0.04895105,  0.85136612]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5000 stopwords= None ngrams= (1, 3)\n",
      "0.745746691871\n",
      "(array([ 0.78571429,  0.74521073]), array([ 0.03971119,  0.99615877]), array([ 0.07560137,  0.85260274]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5000 stopwords= english ngrams= (1, 1)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5000 stopwords= english ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5000 stopwords= english ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5500 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.73333333,  0.74496644]), array([ 0.03971119,  0.99487836]), array([ 0.07534247,  0.85197368]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5500 stopwords= None ngrams= (1, 2)\n",
      "0.741965973535\n",
      "(array([ 0.75      ,  0.74190476]), array([ 0.02166065,  0.99743918]), array([ 0.04210526,  0.85090115]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5500 stopwords= None ngrams= (1, 3)\n",
      "0.743856332703\n",
      "(array([ 0.8       ,  0.74332061]), array([ 0.02888087,  0.99743918]), array([ 0.05574913,  0.8518316 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5500 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5500 stopwords= english ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 5500 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6000 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.73333333,  0.74496644]), array([ 0.03971119,  0.99487836]), array([ 0.07534247,  0.85197368]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6000 stopwords= None ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6000 stopwords= None ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.77777778,  0.74261201]), array([ 0.02527076,  0.99743918]), array([ 0.04895105,  0.85136612]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6000 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6000 stopwords= english ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6000 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6500 stopwords= None ngrams= (1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743856332703\n",
      "(array([ 0.6875    ,  0.74472169]), array([ 0.03971119,  0.99359795]), array([ 0.07508532,  0.85134394]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6500 stopwords= None ngrams= (1, 2)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6500 stopwords= None ngrams= (1, 3)\n",
      "0.743856332703\n",
      "(array([ 0.8       ,  0.74332061]), array([ 0.02888087,  0.99743918]), array([ 0.05574913,  0.8518316 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6500 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6500 stopwords= english ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 6500 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=6500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7000 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.73333333,  0.74496644]), array([ 0.03971119,  0.99487836]), array([ 0.07534247,  0.85197368]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7000 stopwords= None ngrams= (1, 2)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7000 stopwords= None ngrams= (1, 3)\n",
      "0.743856332703\n",
      "(array([ 0.8       ,  0.74332061]), array([ 0.02888087,  0.99743918]), array([ 0.05574913,  0.8518316 ]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7000 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7000 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 1.        ,  0.74239544]), array([ 0.02166065,  1.        ]), array([ 0.04240283,  0.85215494]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7000 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7500 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.73333333,  0.74496644]), array([ 0.03971119,  0.99487836]), array([ 0.07534247,  0.85197368]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7500 stopwords= None ngrams= (1, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7500 stopwords= None ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.77777778,  0.74261201]), array([ 0.02527076,  0.99743918]), array([ 0.04895105,  0.85136612]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7500 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7500 stopwords= english ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 7500 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=7500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8000 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.73333333,  0.74496644]), array([ 0.03971119,  0.99487836]), array([ 0.07534247,  0.85197368]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8000 stopwords= None ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8000 stopwords= None ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.77777778,  0.74261201]), array([ 0.02527076,  0.99743918]), array([ 0.04895105,  0.85136612]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8000 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8000 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 0.875     ,  0.74285714]), array([ 0.02527076,  0.99871959]), array([ 0.04912281,  0.85199345]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8000 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8500 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.73333333,  0.74496644]), array([ 0.03971119,  0.99487836]), array([ 0.07534247,  0.85197368]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8500 stopwords= None ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8500 stopwords= None ngrams= (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742911153119\n",
      "(array([ 0.77777778,  0.74261201]), array([ 0.02527076,  0.99743918]), array([ 0.04895105,  0.85136612]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8500 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8500 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 0.875     ,  0.74285714]), array([ 0.02527076,  0.99871959]), array([ 0.04912281,  0.85199345]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 8500 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=8500, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 9000 stopwords= None ngrams= (1, 1)\n",
      "0.744801512287\n",
      "(array([ 0.73333333,  0.74496644]), array([ 0.03971119,  0.99487836]), array([ 0.07534247,  0.85197368]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 9000 stopwords= None ngrams= (1, 2)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 9000 stopwords= None ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.77777778,  0.74261201]), array([ 0.02527076,  0.99743918]), array([ 0.04895105,  0.85136612]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 9000 stopwords= english ngrams= (1, 1)\n",
      "0.741965973535\n",
      "(array([ 0.83333333,  0.74144487]), array([ 0.01805054,  0.99871959]), array([ 0.03533569,  0.85106383]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 9000 stopwords= english ngrams= (1, 2)\n",
      "0.743856332703\n",
      "(array([ 0.875     ,  0.74285714]), array([ 0.02527076,  0.99871959]), array([ 0.04912281,  0.85199345]), array([277, 781]))\n",
      "Trying vec= TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=9000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) max_features= 9000 stopwords= english ngrams= (1, 3)\n",
      "0.742911153119\n",
      "(array([ 0.85714286,  0.74215033]), array([ 0.02166065,  0.99871959]), array([ 0.04225352,  0.85152838]), array([277, 781]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizers = [CountVectorizer(), TfidfVectorizer()]\n",
    "max_features = np.arange(1000, 9001, 500)\n",
    "stopwords = [None, 'english']\n",
    "ngrams = [(1,1), (1,2), (1,3)]\n",
    "\n",
    "checker(vectorizers, max_features, stopwords, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
